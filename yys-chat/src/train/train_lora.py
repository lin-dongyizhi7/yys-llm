import argparse
from pathlib import Path
import json
import sys
import time
import psutil
import logging
from datetime import datetime

import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    get_linear_schedule_with_warmup,
)
from peft import LoraConfig, TaskType, get_peft_model
from trl import SFTTrainer

# ä¿®æ­£å¯¼å…¥è·¯å¾„ï¼Œå…¼å®¹ç›´æ¥ python æ‰§è¡Œ
ROOT_DIR = Path(__file__).resolve().parents[2]
SRC_DIR = ROOT_DIR / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.append(str(SRC_DIR))

from utils.roles import add_or_update_role  # noqa: E402

# é…ç½®æ—¥å¿—
def setup_logging(output_dir: Path):
    """è®¾ç½®è¯¦ç»†çš„æ—¥å¿—è®°å½•"""
    log_file = output_dir / "training.log"
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

def log_system_info(logger):
    """è®°å½•ç³»ç»Ÿä¿¡æ¯"""
    logger.info("=" * 60)
    logger.info("è®­ç»ƒå¼€å§‹ - ç³»ç»Ÿä¿¡æ¯")
    logger.info("=" * 60)
    
    # Python å’Œ PyTorch ç‰ˆæœ¬
    logger.info(f"Python ç‰ˆæœ¬: {sys.version}")
    logger.info(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    logger.info(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        logger.info(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"GPU æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            logger.info(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
    
    # ç³»ç»Ÿå†…å­˜
    memory = psutil.virtual_memory()
    logger.info(f"ç³»ç»Ÿå†…å­˜: {memory.total / 1024**3:.1f} GB")
    logger.info(f"å¯ç”¨å†…å­˜: {memory.available / 1024**3:.1f} GB")
    logger.info("=" * 60)

def log_model_info(model, tokenizer, logger):
    """è®°å½•æ¨¡å‹ä¿¡æ¯"""
    logger.info("æ¨¡å‹ä¿¡æ¯:")
    logger.info(f"  åŸºç¡€æ¨¡å‹: {type(model).__name__}")
    logger.info(f"  è¯è¡¨å¤§å°: {tokenizer.vocab_size}")
    logger.info(f"  æœ€å¤§é•¿åº¦: {tokenizer.model_max_length}")
    
    # è®¡ç®—æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"  æ€»å‚æ•°: {total_params:,}")
    logger.info(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    logger.info(f"  å‚æ•°å‹ç¼©æ¯”: {trainable_params/total_params*100:.2f}%")
    
    # LoRA é…ç½®ä¿¡æ¯
    if hasattr(model, 'peft_config'):
        for name, config in model.peft_config.items():
            logger.info(f"  LoRA é…ç½® {name}:")
            logger.info(f"    r: {config.r}")
            logger.info(f"    alpha: {config.lora_alpha}")
            logger.info(f"    dropout: {config.lora_dropout}")
            logger.info(f"    target_modules: {config.target_modules}")

def build_lora_config():
    return LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=16,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj"],
    )

def load_tokenizer(base_model: str, logger):
    logger.info(f"æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {base_model}")
    tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.info("è®¾ç½® pad_token = eos_token")
    tokenizer.padding_side = "right"
    logger.info("åˆ†è¯å™¨åŠ è½½å®Œæˆ")
    return tokenizer

def format_dataset(dataset_path: str, logger):
    logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path}")
    
    # dataset æ˜¯ jsonlï¼Œæ¯è¡ŒåŒ…å« role å’Œ dialog
    def to_text(example):
        dialog = example["dialog"]
        # å°†å¤šè½®æ‹¼æ¥ä¸º SFT æ–‡æœ¬ï¼ŒåŠ å…¥å°‘é‡è¡¨æƒ…å¢å¼ºé£æ ¼ ğŸ˜€
        lines = []
        for turn in dialog:
            speaker = "ç”¨æˆ·" if turn["from"] == "user" else "åŠ©æ‰‹"
            text = turn["text"].strip()
            if turn["from"] == "assistant":
                text = text + " ğŸ˜Š"
            lines.append(f"{speaker}: {text}")
        return {"text": "\n".join(lines)}

    ds = load_dataset("json", data_files=dataset_path, split="train")
    logger.info(f"åŸå§‹æ•°æ®é›†å¤§å°: {len(ds)} ä¸ªæ ·æœ¬")
    
    ds = ds.map(to_text)
    logger.info("æ•°æ®é›†æ ¼å¼åŒ–å®Œæˆ")
    
    # ç»Ÿè®¡æ–‡æœ¬é•¿åº¦
    text_lengths = [len(item["text"]) for item in ds]
    avg_length = sum(text_lengths) / len(text_lengths)
    max_length = max(text_lengths)
    min_length = min(text_lengths)
    logger.info(f"æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
    logger.info(f"  å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
    logger.info(f"  æœ€å¤§é•¿åº¦: {max_length} å­—ç¬¦")
    logger.info(f"  æœ€å°é•¿åº¦: {min_length} å­—ç¬¦")
    
    return ds

def pick_precision_flags(logger):
    if not torch.cuda.is_available():
        logger.info("CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU è®­ç»ƒ")
        return {"fp16": False, "bf16": False}
    
    major, _ = torch.cuda.get_device_capability(0)
    if major >= 8:  # Ampere åŠä»¥ä¸Šæ”¯æŒ bf16
        logger.info("ä½¿ç”¨ bfloat16 ç²¾åº¦ (Ampere+ GPU)")
        return {"fp16": False, "bf16": True}
    
    logger.info("ä½¿ç”¨ float16 ç²¾åº¦")
    return {"fp16": True, "bf16": False}

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_model", required=True)
    parser.add_argument("--dataset", required=True)
    parser.add_argument("--role", required=True)
    parser.add_argument("--output_dir", required=True)
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--lr", type=float, default=2e-4)
    parser.add_argument("--cutoff_len", type=int, default=2048)
    parser.add_argument("--quant", choices=["none", "4bit", "8bit"], default="4bit")

    args = parser.parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(output_dir)
    
    # è®°å½•è®­ç»ƒå‚æ•°
    logger.info("è®­ç»ƒå‚æ•°:")
    for key, value in vars(args).items():
        logger.info(f"  {key}: {value}")
    
    # è®°å½•ç³»ç»Ÿä¿¡æ¯
    log_system_info(logger)
    
    start_time = time.time()
    
    torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    logger.info(f"ä½¿ç”¨æ•°æ®ç±»å‹: {torch_dtype}")

    load_kwargs = {}
    if args.quant == "4bit":
        load_kwargs.update(dict(load_in_4bit=True, device_map="auto"))
        logger.info("ä½¿ç”¨ 4bit é‡åŒ– (QLoRA)")
    elif args.quant == "8bit":
        load_kwargs.update(dict(load_in_8bit=True, device_map="auto"))
        logger.info("ä½¿ç”¨ 8bit é‡åŒ–")
    else:
        load_kwargs.update(dict(device_map="auto"))
        logger.info("ä¸ä½¿ç”¨é‡åŒ–")

    # åŠ è½½åˆ†è¯å™¨
    tokenizer = load_tokenizer(args.base_model, logger)
    
    # åŠ è½½æ¨¡å‹
    logger.info(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {args.base_model}")
    model = AutoModelForCausalLM.from_pretrained(args.base_model, torch_dtype=torch_dtype, **load_kwargs)
    logger.info("åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")

    # åº”ç”¨ LoRA é…ç½®
    logger.info("æ­£åœ¨åº”ç”¨ LoRA é…ç½®...")
    lora_config = build_lora_config()
    model = get_peft_model(model, lora_config)
    logger.info("LoRA é…ç½®åº”ç”¨å®Œæˆ")
    
    # è®°å½•æ¨¡å‹ä¿¡æ¯
    log_model_info(model, tokenizer, logger)

    # åŠ è½½å’Œæ ¼å¼åŒ–æ•°æ®é›†
    train_dataset = format_dataset(args.dataset, logger)

    # é€‰æ‹©ç²¾åº¦è®¾ç½®
    prec = pick_precision_flags(logger)

    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    total_steps = len(train_dataset) // args.batch_size * args.epochs
    warmup_steps = int(total_steps * 0.1)  # 10% çš„æ­¥æ•°ç”¨äº warmup
    
    logger.info(f"è®­ç»ƒé…ç½®:")
    logger.info(f"  æ€»æ­¥æ•°: {total_steps}")
    logger.info(f"  Warmup æ­¥æ•°: {warmup_steps}")
    logger.info(f"  æ¯ epoch æ­¥æ•°: {total_steps // args.epochs}")

    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        learning_rate=args.lr,
        fp16=prec["fp16"],
        bf16=prec["bf16"],
        logging_steps=1,  # æ¯æ­¥éƒ½è®°å½•
        save_strategy="epoch",
        evaluation_strategy="no",
        gradient_accumulation_steps=1,
        report_to=[],
        remove_unused_columns=False,
        dataloader_pin_memory=False,
        dataloader_num_workers=0,
        warmup_steps=warmup_steps,
        lr_scheduler_type="linear",
        logging_dir=str(output_dir / "logs"),
        run_name=f"yys-{args.role}-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    logger.info("æ­£åœ¨åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        dataset_text_field="text",
        max_seq_length=args.cutoff_len,
        args=training_args,
    )
    logger.info("è®­ç»ƒå™¨åˆ›å»ºå®Œæˆ")

    # å¼€å§‹è®­ç»ƒ
    logger.info("=" * 60)
    logger.info("å¼€å§‹è®­ç»ƒ!")
    logger.info("=" * 60)
    
    try:
        trainer.train()
        logger.info("è®­ç»ƒå®Œæˆ!")
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise
    
    # ä¿å­˜æ¨¡å‹
    logger.info("æ­£åœ¨ä¿å­˜æ¨¡å‹...")
    trainer.model.save_pretrained(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))
    logger.info("æ¨¡å‹ä¿å­˜å®Œæˆ")

    # è®°å½•è§’è‰²ä¿¡æ¯
    add_or_update_role(args.role, args.base_model, str(output_dir))
    logger.info(f"è§’è‰² '{args.role}' å·²æ³¨å†Œ")

    # ä¿å­˜è®­ç»ƒå…ƒæ•°æ®
    meta_data = {
        "role": args.role,
        "base_model": args.base_model,
        "quant": args.quant,
        "training_args": vars(training_args),
        "dataset_info": {
            "path": args.dataset,
            "size": len(train_dataset),
            "cutoff_len": args.cutoff_len
        },
        "training_time": time.time() - start_time,
        "completed_at": datetime.now().isoformat()
    }
    
    meta_file = output_dir / "meta.json"
    meta_file.write_text(json.dumps(meta_data, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info(f"è®­ç»ƒå…ƒæ•°æ®å·²ä¿å­˜åˆ°: {meta_file}")

    # æœ€ç»ˆç»Ÿè®¡
    total_time = time.time() - start_time
    logger.info("=" * 60)
    logger.info("è®­ç»ƒå®Œæˆæ€»ç»“")
    logger.info("=" * 60)
    logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶ ({total_time/60:.1f} åˆ†é’Ÿ)")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"è§’è‰²åç§°: {args.role}")
    logger.info("=" * 60)

if __name__ == "__main__":
    main()
