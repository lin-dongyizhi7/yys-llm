import gradio as gr
import json
import subprocess
import threading
from pathlib import Path
from datetime import datetime

# é¡¹ç›®æ ¹ç›®å½•
ROOT_DIR = Path(__file__).resolve().parents[2]

def get_datasets():
    """è·å–å¯ç”¨æ•°æ®é›†"""
    data_dir = ROOT_DIR / "data"
    datasets = []
    if data_dir.exists():
        for file in data_dir.rglob("*.jsonl"):
            datasets.append(str(file.relative_to(ROOT_DIR)))
    return datasets

def start_training(base_model, dataset, role, epochs, batch_size, lr, quant):
    """å¯åŠ¨è®­ç»ƒ"""
    output_dir = f"models/{role.lower()}"
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        "python", str(ROOT_DIR / "src" / "train" / "train_lora.py"),
        "--base_model", base_model,
        "--dataset", dataset,
        "--role", role,
        "--output_dir", output_dir,
        "--epochs", str(epochs),
        "--batch_size", str(batch_size),
        "--lr", str(lr),
        "--cutoff_len", "2048",
        "--quant", quant
    ]
    
    try:
        # å¯åŠ¨è®­ç»ƒ
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True
        )
        
        return f"âœ… è®­ç»ƒå·²å¯åŠ¨ï¼\nè§’è‰²: {role}\nè¾“å‡ºç›®å½•: {output_dir}\nè¿›ç¨‹ID: {process.pid}"
    except Exception as e:
        return f"âŒ å¯åŠ¨å¤±è´¥: {str(e)}"

def create_interface():
    """åˆ›å»ºè®­ç»ƒç•Œé¢"""
    
    with gr.Blocks(title="yys-chat è®­ç»ƒç•Œé¢", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸš€ yys-chat æ¨¡å‹è®­ç»ƒ")
        gr.Markdown("é…ç½®å‚æ•°å¹¶å¯åŠ¨ LoRA/QLoRA å¾®è°ƒè®­ç»ƒ")
        
        with gr.Row():
            with gr.Column(scale=2):
                # è®­ç»ƒé…ç½®
                with gr.Group("ğŸ“‹ è®­ç»ƒé…ç½®"):
                    base_model = gr.Dropdown(
                        choices=[
                            "Qwen/Qwen2-1.5B-Instruct",
                            "Qwen/Qwen2-7B-Instruct",
                            "microsoft/DialoGPT-medium",
                            "sshleifer/tiny-gpt2"
                        ],
                        value="Qwen/Qwen2-1.5B-Instruct",
                        label="åŸºç¡€æ¨¡å‹"
                    )
                    
                    dataset = gr.Dropdown(
                        choices=get_datasets(),
                        value="data/yys/train.jsonl",
                        label="è®­ç»ƒæ•°æ®é›†"
                    )
                    
                    role = gr.Textbox(
                        value="Mentor",
                        label="è§’è‰²åç§°"
                    )
                
                # è®­ç»ƒå‚æ•°
                with gr.Group("âš™ï¸ è®­ç»ƒå‚æ•°"):
                    with gr.Row():
                        epochs = gr.Slider(1, 10, 3, step=1, label="è®­ç»ƒè½®æ•°")
                        batch_size = gr.Slider(1, 8, 2, step=1, label="æ‰¹æ¬¡å¤§å°")
                    
                    with gr.Row():
                        lr = gr.Slider(1e-5, 1e-3, 2e-4, step=1e-5, label="å­¦ä¹ ç‡")
                        quant = gr.Radio(["none", "4bit", "8bit"], value="none", label="é‡åŒ–æ–¹å¼")
                
                # æ§åˆ¶æŒ‰é’®
                start_btn = gr.Button("ğŸš€ å¼€å§‹è®­ç»ƒ", variant="primary", size="lg")
                status_output = gr.Textbox(label="çŠ¶æ€", lines=3, interactive=False)
            
            with gr.Column(scale=1):
                # å¿«é€Ÿå¼€å§‹
                with gr.Group("ğŸš€ å¿«é€Ÿå¼€å§‹"):
                    gr.Markdown("**æ¨èé…ç½®:**")
                    gr.Markdown("- æ¨¡å‹: Qwen2-1.5B-Instruct")
                    gr.Markdown("- æ•°æ®é›†: data/yys/train.jsonl")
                    gr.Markdown("- è§’è‰²: Mentor")
                    gr.Markdown("- è½®æ•°: 3")
                    gr.Markdown("- é‡åŒ–: none (CPUè®­ç»ƒ)")
                
                # è®­ç»ƒå†å²
                with gr.Group("ğŸ“š è®­ç»ƒå†å²"):
                    history_btn = gr.Button("ğŸ”„ æŸ¥çœ‹å†å²", size="sm")
                    history_output = gr.JSON(label="å·²å®Œæˆè®­ç»ƒ")
        
        # äº‹ä»¶ç»‘å®š
        start_btn.click(
            start_training,
            inputs=[base_model, dataset, role, epochs, batch_size, lr, quant],
            outputs=status_output
        )
        
        def get_history():
            models_dir = ROOT_DIR / "models"
            history = []
            if models_dir.exists():
                for model_dir in models_dir.iterdir():
                    if model_dir.is_dir():
                        meta_file = model_dir / "meta.json"
                        if meta_file.exists():
                            try:
                                with open(meta_file, 'r', encoding='utf-8') as f:
                                    meta = json.load(f)
                                    history.append({
                                        "è§’è‰²": meta.get("role", "æœªçŸ¥"),
                                        "åŸºç¡€æ¨¡å‹": meta.get("base_model", "æœªçŸ¥"),
                                        "å®Œæˆæ—¶é—´": meta.get("completed_at", "æœªçŸ¥")[:19],
                                        "è®­ç»ƒæ—¶é—´": f"{meta.get('training_time', 0)/3600:.2f}h"
                                    })
                            except:
                                pass
            return history
        
        history_btn.click(get_history, outputs=history_output)
        
        # é¡µé¢åŠ è½½æ—¶è·å–å†å²
        interface.load(get_history, outputs=history_output)
    
    return interface

if __name__ == "__main__":
    interface = create_interface()
    interface.launch(server_port=7861, share=False)
